{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KAVYANSHTYAGI/Ransomware-Analysis-using-Machine-Learning-and-Deep-Learning/blob/main/DL_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO6JYp0PLfxr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau , LearningRateScheduler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfUBKu7EL3_6",
        "outputId": "2edcea15-e243-40b8-ba88-8424d54d92b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGbyb2FcNvvd"
      },
      "outputs": [],
      "source": [
        "# Load your dataset\n",
        "data_path = '/content/drive/MyDrive/ransomware_analysis_files/gan_for_synthetic/balanced_oversampled_very_noisy_extended_5k.csv'\n",
        "dataset = pd.read_csv(data_path)\n",
        "\n",
        "# Check and remove NaN values in the target variable 'Tag_y'\n",
        "if dataset['Tag_y'].isnull().any():\n",
        "    print(\"NaN values found in target variable 'Tag_y', removing rows...\")\n",
        "    dataset = dataset.dropna(subset=['Tag_y'])\n",
        "\n",
        "X = dataset.drop(['Tag_y', 'Tag_x','filename', 'cryptographic_usage_encryption_algorithms','complexity_metrics_function_count', 'data_flow_collections_usage', 'hardcoded_urls', 'obfuscation_techniques_variable_name_length', 'unique_suspicious_strings'], axis=1)\n",
        "y = dataset['Tag_y']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "method 1"
      ],
      "metadata": {
        "id": "sQPHecOYzKiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3QZ3XKdLfxs"
      },
      "outputs": [],
      "source": [
        "# Normalize features to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Reshape data into images\n",
        "num_features = X_scaled.shape[1]\n",
        "next_square = int(np.ceil(np.sqrt(num_features)))  # Calculate the next perfect square\n",
        "total_pixels = next_square**2\n",
        "padding_required = total_pixels - num_features\n",
        "\n",
        "# Pad the features to make the number a perfect square\n",
        "X_padded = np.pad(X_scaled, ((0, 0), (0, padding_required)), 'constant')\n",
        "X_images = X_padded.reshape(-1, next_square, next_square, 1)  # Reshape into (number of samples, height, width, channels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "method 2"
      ],
      "metadata": {
        "id": "yf0ETZl5zMKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Function to convert CSV to image array directly in memory\n",
        "def csv_to_image_array(data, dtype=None, img_dim=(224,224), img_type='RGB'):\n",
        "    images = []\n",
        "    no_of_features = data.shape[1]\n",
        "    sqrt_of_no_of_features = int(np.sqrt(no_of_features))\n",
        "    if np.sqrt(no_of_features) > sqrt_of_no_of_features:\n",
        "        sqrt_of_no_of_features += 1\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        pixels = np.resize(row.values, (sqrt_of_no_of_features**2)).reshape(sqrt_of_no_of_features, sqrt_of_no_of_features)\n",
        "        image = Image.fromarray(pixels.astype(np.uint8))\n",
        "        if img_type != 'Gray':\n",
        "            image = image.convert(img_type)\n",
        "        image = image.resize(img_dim, Image.Resampling.NEAREST)\n",
        "        images.append(np.array(image))\n",
        "    return np.array(images)\n",
        "\n",
        "# Load your dataset\n",
        "data_path = '/content/drive/MyDrive/ransomware_analysis_files/gan_for_synthetic/balanced_oversampled_very_noisy_extended_5k.csv'\n",
        "dataset = pd.read_csv(data_path)\n",
        "\n",
        "# Check for NaN values in the target variable and remove them if found\n",
        "if dataset['Tag_y'].isnull().any():\n",
        "    print(\"NaN values found in target variable 'Tag_y', removing rows...\")\n",
        "    dataset = dataset.dropna(subset=['Tag_y'])\n",
        "\n",
        "# Dropping unnecessary columns\n",
        "features_to_drop = ['Tag_y', 'Tag_x', 'filename', 'cryptographic_usage_encryption_algorithms',\n",
        "                    'complexity_metrics_function_count', 'data_flow_collections_usage', 'hardcoded_urls',\n",
        "                    'obfuscation_techniques_variable_name_length', 'unique_suspicious_strings']\n",
        "X_data = dataset.drop(columns=features_to_drop)\n",
        "y_train = dataset['Tag_y'].values  # Assuming y_train corresponds to the rows in the CSV\n",
        "\n",
        "# Convert the filtered CSV data into images\n",
        "X_train = csv_to_image_array(X_data, dtype='uint8', img_dim=(224,224), img_type='RGB')\n",
        "\n",
        "# Optionally, verify the shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n"
      ],
      "metadata": {
        "id": "sQzTC_9a8BRD",
        "outputId": "a4fd6347-cc99-4451-d41f-934004f45979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ea390ae9c461>:15: RuntimeWarning: invalid value encountered in cast\n",
            "  image = Image.fromarray(pixels.astype(np.uint8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (12012, 224, 224, 3)\n",
            "y_train shape: (12012,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Function to resize images to a new size using TensorFlow\n",
        "def resize_images(images, new_size=(224, 224)):\n",
        "    # Ensure images are a tensor before resizing\n",
        "    images_tensor = tf.convert_to_tensor(images, dtype=tf.float32)\n",
        "    return tf.image.resize(images_tensor, new_size)\n",
        "\n",
        "# Resize X_train to (224, 224)\n",
        "X_train_resized = resize_images(X_train)\n",
        "\n",
        "# Convert y_train to categorical\n",
        "y_train_categorical = to_categorical(y_train)\n",
        "\n",
        "# Print shapes to verify everything is as expected\n",
        "print(\"Resized X_train shape:\", X_train_resized.shape)\n",
        "print(\"Categorical y_train shape:\", y_train_categorical.shape)\n"
      ],
      "metadata": {
        "id": "ziJVsApFGY9R",
        "outputId": "107edab1-e117-47e0-9851-3d57781e52ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized X_train shape: (12012, 224, 224, 3)\n",
            "Categorical y_train shape: (12012, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIVoMKstltk7",
        "outputId": "a315ce22-17a5-4450-b9b9-1dd983ca0d26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (9609, 224, 224, 3)\n",
            "X_val shape: (1201, 224, 224, 3)\n",
            "X_test shape: (1202, 224, 224, 3)\n",
            "y_train shape: (9609, 2)\n",
            "y_val shape: (1201, 2)\n",
            "y_test shape: (1202, 2)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_data(X, y, train_size, val_size, test_size):\n",
        "    # Convert TensorFlow tensors to numpy arrays if they are not already\n",
        "    if isinstance(X, tf.Tensor):\n",
        "        X = X.numpy()\n",
        "    if isinstance(y, tf.Tensor):\n",
        "        y = y.numpy()\n",
        "\n",
        "    # First split to separate out the training set\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, random_state=42)\n",
        "\n",
        "    # Calculate the proportion of the remaining data to allocate to validation\n",
        "    # This calculation ensures that the validation and test sets are of the correct relative size\n",
        "    prop_remain = val_size / (val_size + test_size)\n",
        "\n",
        "    # Second split to separate out the validation and test sets\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=prop_remain, random_state=42)\n",
        "\n",
        "    # Convert arrays back to tensors if necessary\n",
        "    X_train, X_val, X_test = map(tf.convert_to_tensor, [X_train, X_val, X_test])\n",
        "    y_train, y_val, y_test = map(tf.convert_to_tensor, [y_train, y_val, y_test])\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Assuming X_train_resized and y_train_categorical are ready and correctly formatted as tensors\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_train_resized, y_train_categorical, 0.8, 0.1, 0.1)\n",
        "\n",
        "# Print the shapes of the datasets to verify the splits\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FJiONFpLfxs"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation configuration\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=70,        # Random rotation between -10 to +10 degrees\n",
        "    width_shift_range=0.1,    # Horizontal shift\n",
        "    height_shift_range=0.1,   # Vertical shift\n",
        "    zoom_range=0.1,           # Random zoom\n",
        "    horizontal_flip=True,     # Horizontal flip\n",
        "    fill_mode='nearest'  ,\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow(X_train, y_train, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom layers"
      ],
      "metadata": {
        "id": "WKADNdtdHgkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhINMXs8Lfxt"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(next_square, next_square, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(y_categorical.shape[1], activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model with data augmentation\n",
        "model.fit(datagen.flow(X_train_resized, y_train, batch_size=64),\n",
        "          epochs=30,\n",
        "          validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Pre-trained models"
      ],
      "metadata": {
        "id": "qPYClKZxHfjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6_8ZI9VLfxt",
        "outputId": "e8d67b32-80c2-4fff-f16c-07f447953b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "# Using MobileNetV2 as the base model with pre-trained ImageNet weights\n",
        "base_model = InceptionResNetV2(include_top=False,\n",
        "                         weights='imagenet',\n",
        "                         input_shape=(224, 224, 3))\n",
        "\n",
        "# Setting the base model as non-trainable\n",
        "base_model.trainable = True\n",
        "\n",
        "# Building the complete model\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Configuring the optimizer\n",
        "optimizer = Adam(learning_rate=1e-4)\n",
        "\n",
        "# Compiling the model with necessary parameters\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',  # This is important for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setting up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-6)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# The model is now ready to be trained with model.fit(), ensuring to provide validation data and using the callbacks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid Model"
      ],
      "metadata": {
        "id": "Ed_pTpyvHsns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50, VGG19\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "# Input shape\n",
        "input_shape = (224, 224, 3)\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "# Load ResNet50 and VGG19 pre-trained models\n",
        "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "vgg_base = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Set base models as trainable (or freeze as required)\n",
        "resnet_base.trainable = False\n",
        "vgg_base.trainable = False\n",
        "\n",
        "# Extract features\n",
        "resnet_features = GlobalAveragePooling2D()(resnet_base(inputs))\n",
        "vgg_features = GlobalAveragePooling2D()(vgg_base(inputs))\n",
        "\n",
        "# Concatenate features\n",
        "combined_features = Concatenate()([resnet_features, vgg_features])\n",
        "\n",
        "# Add a classification head\n",
        "x = Dense(512, activation='relu')(combined_features)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "outputs = Dense(num_classes, activation='sigmoid')(x)\n",
        "\n",
        "# Build the model\n",
        "hybrid_model = Model(inputs, outputs)\n",
        "\n",
        "# Configure the optimizer\n",
        "optimizer = Adam(learning_rate=1e-4)\n",
        "\n",
        "# Compile the model\n",
        "hybrid_model.compile(optimizer=optimizer,\n",
        "                     loss='categorical_crossentropy',  # For multi-class classification\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "# Setup callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-6)\n",
        "model_checkpoint = ModelCheckpoint('best_hybrid_model.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Model summary\n",
        "hybrid_model.summary()\n"
      ],
      "metadata": {
        "id": "VQMgi_zqH9Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrw9Qz_VLfxu",
        "outputId": "05602f17-e955-4262-ecdc-47199468078a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train: (9609, 224, 224, 3)\n",
            "Shape of X_test: (721, 224, 224, 3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:1263: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:1273: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 224, 224, 3) (64, 2)\n",
            "Batch X size: (64, 224, 224, 3) Batch y size: (64, 2)\n",
            "Validation X size: (1682, 224, 224, 3) Validation y size: (1682, 2)\n",
            "test X size: (721, 224, 224, 3) test y size: (721, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "\n",
        "for X_batch, y_batch in train_generator:\n",
        "    print(X_batch.shape, y_batch.shape)\n",
        "    break  # Just run one iteration to check\n",
        "\n",
        "for X_batch, y_batch in train_generator:\n",
        "    print('Batch X size:', X_batch.shape, 'Batch y size:', y_batch.shape)\n",
        "    if X_batch.shape[0] != y_batch.shape[0]:\n",
        "        print('Mismatch found')\n",
        "    break  # Stop after the first batch to check\n",
        "\n",
        "print('Validation X size:', X_val.shape, 'Validation y size:', y_val.shape)\n",
        "print('test X size:', X_test.shape, 'test y size:', y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS7HAC_mLfxu",
        "outputId": "f106df95-1421-43cf-b9d6-c9cf1ffcabf0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:1263: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:1273: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 3s/step - accuracy: 0.6962 - loss: 0.5527 - val_accuracy: 0.7510 - val_loss: 0.4762 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - accuracy: 0.8866 - loss: 0.2815 - val_accuracy: 0.8235 - val_loss: 0.4490 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9062 - loss: 0.2244 - val_accuracy: 0.8901 - val_loss: 0.2824 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9321 - loss: 0.1821 - val_accuracy: 0.9251 - val_loss: 0.1946 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9342 - loss: 0.1692 - val_accuracy: 0.9326 - val_loss: 0.1674 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9417 - loss: 0.1521 - val_accuracy: 0.9475 - val_loss: 0.1506 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9458 - loss: 0.1451 - val_accuracy: 0.9575 - val_loss: 0.1113 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - accuracy: 0.9503 - loss: 0.1352 - val_accuracy: 0.9484 - val_loss: 0.1352 - learning_rate: 1.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9542 - loss: 0.1231 - val_accuracy: 0.9442 - val_loss: 0.1470 - learning_rate: 1.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.9521 - loss: 0.1341 - val_accuracy: 0.9559 - val_loss: 0.1080 - learning_rate: 1.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9579 - loss: 0.1130 - val_accuracy: 0.9434 - val_loss: 0.1416 - learning_rate: 1.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9554 - loss: 0.1179 - val_accuracy: 0.9509 - val_loss: 0.1186 - learning_rate: 1.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9579 - loss: 0.1105 - val_accuracy: 0.9559 - val_loss: 0.1147 - learning_rate: 1.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9593 - loss: 0.1085 - val_accuracy: 0.9484 - val_loss: 0.1499 - learning_rate: 1.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.9549 - loss: 0.1177 - val_accuracy: 0.9634 - val_loss: 0.0912 - learning_rate: 2.0000e-05\n",
            "Epoch 16/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9684 - loss: 0.0842 - val_accuracy: 0.9642 - val_loss: 0.0893 - learning_rate: 2.0000e-05\n",
            "Epoch 17/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9694 - loss: 0.0813 - val_accuracy: 0.9684 - val_loss: 0.0850 - learning_rate: 2.0000e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9725 - loss: 0.0741 - val_accuracy: 0.9692 - val_loss: 0.0790 - learning_rate: 2.0000e-05\n",
            "Epoch 19/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.9775 - loss: 0.0659 - val_accuracy: 0.9725 - val_loss: 0.0770 - learning_rate: 2.0000e-05\n",
            "Epoch 20/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 1s/step - accuracy: 0.9745 - loss: 0.0680 - val_accuracy: 0.9709 - val_loss: 0.0788 - learning_rate: 2.0000e-05\n",
            "Epoch 21/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.9740 - loss: 0.0673 - val_accuracy: 0.9709 - val_loss: 0.0749 - learning_rate: 2.0000e-05\n",
            "Epoch 22/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9761 - loss: 0.0647 - val_accuracy: 0.9692 - val_loss: 0.0803 - learning_rate: 2.0000e-05\n",
            "Epoch 23/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9749 - loss: 0.0705 - val_accuracy: 0.9734 - val_loss: 0.0771 - learning_rate: 2.0000e-05\n",
            "Epoch 24/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9757 - loss: 0.0645 - val_accuracy: 0.9742 - val_loss: 0.0729 - learning_rate: 2.0000e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.9738 - loss: 0.0697 - val_accuracy: 0.9750 - val_loss: 0.0711 - learning_rate: 2.0000e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9768 - loss: 0.0644 - val_accuracy: 0.9700 - val_loss: 0.0760 - learning_rate: 2.0000e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - accuracy: 0.9787 - loss: 0.0595 - val_accuracy: 0.9667 - val_loss: 0.0843 - learning_rate: 2.0000e-05\n",
            "Epoch 28/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - accuracy: 0.9778 - loss: 0.0596 - val_accuracy: 0.9709 - val_loss: 0.0793 - learning_rate: 2.0000e-05\n",
            "Epoch 29/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9777 - loss: 0.0643 - val_accuracy: 0.9692 - val_loss: 0.0703 - learning_rate: 2.0000e-05\n",
            "Epoch 30/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9787 - loss: 0.0594 - val_accuracy: 0.9700 - val_loss: 0.0735 - learning_rate: 2.0000e-05\n",
            "Epoch 31/100\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.9804 - loss: 0.0526 - val_accuracy: 0.9700 - val_loss: 0.0697 - learning_rate: 2.0000e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m58/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9811 - loss: 0.0462"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
        "    shuffle=True)\n",
        "\n",
        "# Plotting training and validation loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'val Loss: {loss}, val Accuracy: {accuracy}')\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'test Loss: {loss}, test Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRID SEARCH FOR HYPERPARAMETER TUNING"
      ],
      "metadata": {
        "id": "HRIEaarLtAEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy-UyGE_Lfxu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50, VGG19\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameter grid\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "dense_1_units = [256, 512]\n",
        "dropout_1_rates = [0.2, 0.3]\n",
        "dense_2_units = [128, 256]\n",
        "dropout_2_rates = [0.2, 0.3]\n",
        "\n",
        "# Input shape\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Grid Search Loop\n",
        "best_model = None\n",
        "best_val_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Dummy data for demonstration\n",
        "x_train = np.random.rand(100, 224, 224, 3)\n",
        "y_train = np.random.randint(0, 2, size=(100, 2))\n",
        "x_val = np.random.rand(20, 224, 224, 3)\n",
        "y_val = np.random.randint(0, 2, size=(20, 2))\n",
        "\n",
        "for lr, d1_units, d1_drop, d2_units, d2_drop in product(learning_rates, dense_1_units, dropout_1_rates, dense_2_units, dropout_2_rates):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape, trainable=True)\n",
        "    vgg_base = VGG19(weights='imagenet', include_top=False, input_shape=input_shape, trainable=True)\n",
        "\n",
        "    resnet_features = GlobalAveragePooling2D()(resnet_base(inputs))\n",
        "    vgg_features = GlobalAveragePooling2D()(vgg_base(inputs))\n",
        "\n",
        "    combined_features = Concatenate()([resnet_features, vgg_features])\n",
        "\n",
        "    x = Dense(d1_units, activation='relu')(combined_features)\n",
        "    x = Dropout(d1_drop)(x)\n",
        "    x = Dense(d2_units, activation='relu')(x)\n",
        "    x = Dropout(d2_drop)(x)\n",
        "    outputs = Dense(2, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, verbose=0)\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    val_accuracy = max(history.history['val_accuracy'])\n",
        "    print(f\"Params: lr={lr}, d1_units={d1_units}, d1_drop={d1_drop}, d2_units={d2_units}, d2_drop={d2_drop}, val_acc={val_accuracy:.4f}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model = model\n",
        "        best_params = {'learning_rate': lr, 'dense_1_units': d1_units, 'dropout_1_rate': d1_drop, 'dense_2_units': d2_units, 'dropout_2_rate': d2_drop}\n",
        "\n",
        "print(\"Best Validation Accuracy:\", best_val_accuracy)\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENETIC ALGORITHM"
      ],
      "metadata": {
        "id": "0bcQWAdO1ILc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wivCSiFxLfxu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "# Fitness function\n",
        "def evaluate(params):\n",
        "    lr, d1_units, d1_drop, d2_units, d2_drop = params\n",
        "\n",
        "    # Build model\n",
        "    inputs = Input(shape=input_shape)\n",
        "    resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape, trainable=True)\n",
        "    vgg_base = VGG19(weights='imagenet', include_top=False, input_shape=input_shape, trainable=True)\n",
        "\n",
        "    resnet_features = GlobalAveragePooling2D()(resnet_base(inputs))\n",
        "    vgg_features = GlobalAveragePooling2D()(vgg_base(inputs))\n",
        "\n",
        "    combined_features = Concatenate()([resnet_features, vgg_features])\n",
        "\n",
        "    x = Dense(d1_units, activation='relu')(combined_features)\n",
        "    x = Dropout(d1_drop)(x)\n",
        "    x = Dense(d2_units, activation='relu')(x)\n",
        "    x = Dropout(d2_drop)(x)\n",
        "    outputs = Dense(2, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, verbose=0)\n",
        "    return max(history.history['val_accuracy']),\n",
        "\n",
        "# Genetic Algorithm Setup\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_lr\", random.choice, [1e-3, 1e-4])\n",
        "toolbox.register(\"attr_d1_units\", random.choice, [256, 512])\n",
        "toolbox.register(\"attr_d1_drop\", random.uniform, 0.2, 0.5)\n",
        "toolbox.register(\"attr_d2_units\", random.choice, [128, 256])\n",
        "toolbox.register(\"attr_d2_drop\", random.uniform, 0.2, 0.5)\n",
        "\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "                 (toolbox.attr_lr, toolbox.attr_d1_units, toolbox.attr_d1_drop, toolbox.attr_d2_units, toolbox.attr_d2_drop), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "\n",
        "# Run GA\n",
        "population = toolbox.population(n=10)\n",
        "algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=5, verbose=True)\n",
        "\n",
        "# Print best individual\n",
        "best_individual = tools.selBest(population, 1)[0]\n",
        "print(\"Best Hyperparameters:\", best_individual)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}